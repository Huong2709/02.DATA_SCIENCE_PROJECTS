{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BuiThiHuong_P2_Step0_Abstract_BESTMODEL.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMYFd1F2TZwUYPGXCVmmR2D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Step 1: BUSINESS UNDERSTANDING"],"metadata":{"id":"3X3N5B-wLenZ"}},{"cell_type":"markdown","source":["* On foody.vn, there are many restaurants doing business with diverse and rich products and menus. To increase the level of interaction with customers, each restaurant has a comment area for customers to give reviews, comments and scores.Instead of having to conduct customer survey campaigns, analyzing customer comments for food on foody.vn will be faster and more effective.\n","* Sentiment analysis is a complicated and sometimes personal opinion. Manual analysis (ie reading each comment and classification) will give inconsistent results between the analysts and take a lot of time. \n","* Natural language analysis algorithms, especially sentiment, are very developed. The application of NLP algorithms to predict customer sentiment will help restaurants to improve service quality in a timely manner."],"metadata":{"id":"n4FgmzNjLjmP"}},{"cell_type":"markdown","source":["#Step 2: DATA UNDERSTANDING"],"metadata":{"id":"HrcI0kQQLj8q"}},{"cell_type":"markdown","source":["* Dataset has 3 columns: restaurant, review_text, review_score. \n","* review_score: from 1 to 10. The higher the score, the higher the satisfaction.\n","* Create new feature \"review_score_new\": class 1 if review_score < 7, class 0 if review_score >= 7. Find out the imbalance between classes. Class 0 is more than class 1. But we focus on class 1 (dislike)\n","* review_text: Vietnamese language.\n","* Use Underthesea and CountVectorizer to transform data to bag_of_words.\n","* Use WordCloud to view important words.\n","* Save all dataset to csv files.\n","* Note: don't remove additional stopwords. Train models with original data, and then remove stopwords and train models again. "],"metadata":{"id":"4vYLVP-bLoY9"}},{"cell_type":"markdown","source":["#Step 3: DATA PREPARATION"],"metadata":{"id":"vTZE_OxtejtF"}},{"cell_type":"markdown","source":["* Do Train Test spliting with rate = 70:30 => X_train, X_test, y_train, y_test\n","* Do Undersampling => X_train_us, X_test_us, y_train_us, y_test_us\n","* Do Oversampling => X_train_os, X_test_os, y_train_os, y_test_os\n","* Save all dataset to csv files.\n","* Note: when buidl models and evaluation, if can use these saved dataset, use them, if need to do Data Preparation again, do gain."],"metadata":{"id":"z-jNKOrjevO0"}},{"cell_type":"markdown","source":["# Step 4&5: MODELING & EVALUATION/ ANALYZE AND REPORT"],"metadata":{"id":"xGVgfFyyevRl"}},{"cell_type":"markdown","source":["## 2A: Use postag, don't remove additional stopwords:\n","* Build LazyClassifier to identify what models will be suitable.\n","* According to ROC AUC, F1-score, Accuracy of general models of LazyClassifier, the Naive Bayes models and Tree models are the suiltable.\n","* Build and evaluate each models: MultinomialMB, BernoulliNB, LGBMClassifier, ExtraTreesClassifier, RandomForestClassifier.\n","* Build an RNN LSTM model.\n","\n","* **Conclusion:** If focus on recall score of class 1 (dislike), there are 2 models are good:\n","\n"," MultinomialMB: with recall score of class 1 = 74%, recall score of class 0 = 89%, Macro F1-score = 80% (precision of class 1 = 65%, precision of class 0 = 92%)\n"," \n"," ExtraTreesClassifier: with recall score of class 1 = 77%, recall score of class 0 = 87%, Macro F1-score = 80% (precision of class 1 = 63%, precision of class 0 = 93%)"],"metadata":{"id":"7fkVMBJlevUQ"}},{"cell_type":"markdown","source":["# TRY AGAIN STEP 3,4,5 WITH SOME OPTIONS:"],"metadata":{"id":"YMItY8cbmQbT"}},{"cell_type":"markdown","source":["## 2B: Try to **remove more stopwords**:\n","* Because of the long time to transform data. I selected about 2500 value words that related to sentiment. These other words are not value for sentiment.\n","* Use saved X_train_us, X_test_us, X_train_os, X_test_os (after undersampling and oversamplin) on the folder already and drop columns that isn't the value words. So just about 2500 columns remained.\n","* Buld models again with clean data.\n","* **Conclusion:** If focus on recall score of class 1 (dislike), the MultinomialMB is the best model, with recall score of class 1 = 73%, recall score of class 0 = 87% and Macro F1-score = 78% (precision of class 1 = 62%, precision of class 0 = 92%). So after removing stopwords, the model is not improved.\n","* Note: see file jpynb with suffix: \"_stopwords\""],"metadata":{"id":"o-RcsfCWmxwI"}},{"cell_type":"markdown","source":["## 2C: Try Vietnamese **without accents**:\n","* Don't use Underthesea, just conver to Vietnamese without accents.\n","* Do again all 3,4,5 steps, but with data withoutaccents.\n","* **Conclusion:** If focus on recall score of class 1 (dislike), there are 2 models are good:\n","\n"," MultinomialMB: with recall score of class 1 = 74%, recall score of class 0 = 87%, Macro F1-score = 79% (precision of class 1 = 63%, precision of class 0 = 92%)\n"," \n"," ExtraTreesClassifier: with recall score of class 1 = 77%, recall score of class 0 = 88%, Macro F1-score = 81% (precision of class 1 = 66%, precision of class 0 = 93%)\n","\n","* Note: see jpynb file with suffix: **\"_noAccent\"**"],"metadata":{"id":"gvpxit5jlkF1"}},{"cell_type":"markdown","source":["## 2D: Try to define **my own text processing**:\n","* Use only word_tokenizer from Underthesea library, and define next text cleaning steps by my self.\n","* Convert emojicons to words: Extract enojicons from text dataset, scan each emojicon and define to the meaning words. Load edited emojicons for data cleaning.\n","* Remove some special symbols: List down list of special symbols, such as: '~','`','!','@','#','$',... Remove these symbols.\n","* Remove some typing mistakes: \" \" (2 spaces), \" \" (1 space, 1 underscore), \" \" (1 underscore, 1 space)\n","* Link some special words with other words: Link \"không\", \"ko\", \"kg\", \"chả\", \"chẳng\" to the word right behind. Ex: \"không thích\" => \"không_thích\"\n","* Remove stopwords:\n","1. First, do all above text cleaning steps and use CountVectorize to convert to bag of words.\n","2. Save this bag of words to csv file.\n","3. Scan each words and define what words will be removed, what words will be remained.\n","4. Save the chosen words to txt file.\n","5. Load chosen words, if words in document are in chosen words, they will be remaned. If not, they will be removed.\n","\n","* **Conclusion**:If focus on recall score of class 1 (dislike), the MultinomialMB is the best model, with recall score of class 1 = 78%, recall score of class 0 = 90% and Macro F1-score = 82% (precision of class 1 = 66%, precision of class 0 = 94%)* \n","\n","* Note: see jpynb file with suffix: **\"_self\"**"],"metadata":{"id":"lFDX1vCa3VNR"}},{"cell_type":"markdown","source":["## 2E: Try **crawling additional data** from foody.vn: \n","* Crawling data from foody.vn by selenium on jupyter notebook: about 18,948 samples. Append to existing data, we have 58,873 samples.\n","* Do data pre-processing by my own text processing.\n","* Append new clean crawling data to exiting data\n","* Build model again.\n","* Conclusion: If focus on recall score of class 1 (dislike), the MultinomialMB is the best model, with recall score of class 1 = 74%, recall score of class 0 = 89% and Macro F1-score = 80% (precision of class 1 = 66%, precision of class 0 = 93%).\n","* But this result is not better than the previous model (at step 2D)\n","* Note: see the jpynb file with suffix: \"_crawling\""],"metadata":{"id":"xiZKl_oGpGPJ"}},{"cell_type":"markdown","source":["# Step 6: MODELING & EVALUATION/ ANALYZE AND REPORT"],"metadata":{"id":"Q2Xoe_sjpf8a"}},{"cell_type":"markdown","source":["* Choose MultinomialMB model at option 2D, because this model has highest preformance, specially recall of class 1 (dislike).\n","* Create a pipeline that helps end-user can use models easily.\n","* Create a new jpynb and use pipeline to predict data."],"metadata":{"id":"BjQ9DGJSHWyw"}},{"cell_type":"code","source":[""],"metadata":{"id":"gnqzkBFoOeh3"},"execution_count":null,"outputs":[]}]}